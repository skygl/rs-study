{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "\n",
    "## Author\n",
    "\n",
    "- Yoshua Bengio\n",
    "- Réjean Ducharme\n",
    "- Pascal Vincent\n",
    "- Christian Jauvin\n",
    "\n",
    "## Abstract\n",
    "\n",
    "\n",
    "통계적 언어 모델링의 목적은 언어에서 단어 시퀀스들의 결합 확률 함수 `Joint Probability Function` 를 배우는 것입니다. 하지만 이것은 <font color='blue'>**차원의 저주 `Curse of Dimensionality`**</font> 때문에 본질적으로 어렵습니다. \n",
    "\n",
    "우리는 차원의 저주 문제를 각 트레이닝 시퀀스가 모델에게 많은 수의 의미상 가까운 문장들에 대해 알려주는 <font color='blue'>**분산 표현 `Distributed Representation`**</font> 을 학습함으로써 해결하는 것을 제안합니다.\n",
    "\n",
    "모델은 단어 시퀀스들의 확률 함수와 각 단어의 분산 표현을 동시에 배웁니다. 새로운 문장을 구성하는 단어 시퀀스들이 이미 봤던 문장을 구성하는 단어 시퀀스들과 비슷하게 구성되어 있다면 더 높은 확률을 가지기 때문에 일반화할 수 있습니다.\n",
    "\n",
    "### Keyword\n",
    "\n",
    "- Statistical Language Modeling\n",
    "- Artificial Neural Networks\n",
    "- Distributed Representation\n",
    "- Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "언어 모델링과 다른 학습 문제들을 어렵게 만드는 근본적인 문제는 차원의 저주입니다. 이 문제는 여러 이산적인 랜덤 변수들(예를 들면, 문장안의 단어들 또는 데이터마이닝 작업의 이산적인 속성들) 사이에서 결합 분포를 모델링하는 경우에 명백해집니다.\n",
    "\n",
    "(예를들어 Multi-Layer Neural Networks 혹은 Gaussian Mixture Models과 같은 함수들의 평활한 클래스들에서) 연속적인 변수들을 모델링할 때 학습할 함수가 어떤 부분에서 평활성(Smoothness) 성질들을 가지고 있기 때문에 우리는 더 쉽게 일반화할 수 있습니다.\n",
    "\n",
    "이산적인 공간에서 일반화 구조는 명백하지 않습니다. 이러한 이산적인 변수들의 변화는 예상된 함수의 값에 극단적인 영향을 주고, 각각의 변수들이 가질 수 있는 값들의 수가 많을 때 대부분의 관측값들은 해밍 거리에서 거의 최대로 멀어집니다.\n",
    "\n",
    "[Non-Parametric 밀도 추정](#ref-1)의 관점에서 영감을 받아 학습 알고리즘들이 얼마나 다르게 일반화하는지를 시각화하는 좋은 방법은 처음에 훈련 지점(예를 들면 훈련 문장들)에 집중된 [확률 질량(Probability Mass)](#ref-2)이 더 넓게 분산되는지(주로 훈련 지점 주변의 이웃들의 형태로)를 생각하는 것입니다.\n",
    "\n",
    "통계적 언어 모델은 이전의 모든 단어들이 주어지는 다음 단어의 조건부 확률로 표현될 수 있습니다. [더 자세히 알아보기](#SLM)\n",
    "$$\\hat{P}(w_1^T)=\\prod_{t=1}^T{\\hat{P}(w_t|w_1^{t−1})}$$\n",
    "- $w_t$ : t번째 단어\n",
    "- $w_i^j=(w_i, w_{i+1},\\cdot\\cdot\\cdot,w_{j-1},w_j)$\n",
    "자연어 통계적 모델을 구성할 때, 단어 순서를 이용하여 이 모델링 문제의 난이도를 상당히 줄이며, 단어 순서에서 일시적으로 더 가까운 단어가 통계적으로 더 의존적이라는 사실을 알게 됩니다.\n",
    "\n",
    "그러므로 n-gram 모델은 다음 단어에 대한 조건부 확률의 테이블들을 구축합니다.\n",
    "\n",
    "$$\\hat{P}(w_t|w_1^{t-1})\\approx\\hat{P}(w_t|w_{t-n+1}^{t-1})$$\n",
    "\n",
    "우리는 훈련 데이터에서 실제로 발생하거나 충분히 자주 발생하는 연속적인 단어의 조합들만을 고려합니다.\n",
    "\n",
    "만약 훈련 데이터에서 볼 수 없었던 새로운 n 단어의 조합이 발생하면 어떨까요? 이러한 새로운 조합들이 발생할 수 있고, 더 큰 context 사이즈에서는 더 자주 발생할 수 있기 때문에 0이라는 확률을 만들지 않습니다. 작은 Context 크기를 사용하여 확률을 예측하는 것은 다음 모델들에서 찾을 수 있습니다.\n",
    "\n",
    "- Back-off Trigram Models by Katz, 1987\n",
    "- Smoothed Trigram Models by Jelinek and Mercer, 1980\n",
    "\n",
    "본질적으로, 새로운 문장은 훈련 데이터에서 자주 볼 수 있었던 단어들의 조각들을 이어붙여져서 만들어집니다. 다음 조각의 확률을 얻는 규칙은 Back-off 혹은 Interpolated N-그램 알고리즘에 포함되어 있습니다. 분명 문장에는 앞의 몇 개의 단어들보다 단어를 예측할 수 있는 더 많은 정보들이 있습니다.\n",
    "\n",
    "이 접근법은 적어도 두 가지의 특징이 있으며, 우리는 이 논문에서 이것들을 다룰 것입니다.\n",
    "\n",
    "1. 1, 2개 이상의 단어의 문맥을 고려하지 않습니다.\n",
    "2. 단어들 사이에 유사성을 고려하지 않습니다.\n",
    "\n",
    "우리는 먼저 제안된 접근법에 대한 기본적인 아이디어들을 논의할 것입니다. 그 후 Multi-Layer Neural Networks에 기반한 아이디어의 구현을 사용하여 발표할 것입니다. 그 다음 매우 큰 데이터 셋에 대한 뉴럴 네트워크 훈련을 고려할 것입니다. 마지막으로 대규모의 모델과 같은 훈련이 비용이 크지만 실현 가능하고, 대규모의 문맥으로 확장할 수 있고, 좋은 비교 결과를 산출하는 것을 보여줍니다.\n",
    "\n",
    "## Fighting the Curse of Dimensionality with Distributed Representations\n",
    "\n",
    "nutshell에서 제안된 접근법에 대한 아이디어는 다음과 같이 요약될 수 습니다.\n",
    "\n",
    "1. 사전에 있는 각 단어들을 분산된 단어 피처 벡터로 연관짓습니다.\n",
    "2. 문장의 결합 확률 함수를 문장 안에 있는 단어들의 피처 벡터로 표현합니다.\n",
    "3. 단어 특징 벡터들과 확률 함수의 파라미터들을 동시에 학습합니다.\n",
    "\n",
    "피처 벡터는 단어의 다른 측면을 표현합니다. 각각의 단어들은 벡터 공간에서의 한 점으로 연관됩니다. 특징들의 갯수는 단어의 크기에 비해 매우 작습니다. \n",
    "\n",
    "확률 함수는 이전 단어들이 주어졌을 때 다음 단어의 조건부 확률들의 곱으로 표현됩니다. 이 함수는 가중치 감수 페널티를 추가하는 것과 같이 훈련 데이터의 로그 가능성 또는 정규화된 기준을 최대화하기 위해 반복적으로 튜닝될 수 있는 파라미터들을 가지고 있습니다. 각각의 단어들과 연관된 피처 벡터들은 학습되지만 의미적인 사전 지식을 사용하여 초기화될 수 있습니다.\n",
    "\n",
    "제안된 모델에서는 비슷한 단어들은 비슷한 피쳐 벡터를 가지는 것으로 예상되고 확률 함수는 피쳐에서 작은 차이가 확률에서도 작은 차이로 이어지는 이러한 피쳐 값들의 Smooth 함수이기 때문에 일반화될 수 있습니다.\n",
    "\n",
    "## Relation to Previous Work\n",
    "\n",
    "고차원의 이산적인 분산을 모델링하기 위해 뉴럴 네트워크를 사용하는 아이디어는 $Z_1\\cdot\\cdot\\cdot Z_n$이라는 각각 성질이 다른 랜덤 변수들의 집합의 결합 확률을 학습하는데 유용하다는 것은 이미 밝혀졌습니다. (Bengio, 2000)\n",
    "\n",
    "그 모델에서 결합 확률은 조건부 확률들의 곱으로 분해되었습니다.\n",
    "\n",
    "$$\\hat{P}(Z_1=z_1,\\cdot\\cdot\\cdot,Z_n=z_n)=\\prod_i\\hat{P}(Z_i=z_i|g_i(Z_{i-1}=z_{i-1},Z_{i-2}=z_{i-2},\\cdot\\cdot\\cdot,Z_1=z_1))$$\n",
    "\n",
    "$g(.)$는 특별한 left-to-right 아키텍쳐에서의 뉴럴 네트워크에서 표현되는 함수이고, i번째 결과 블록 $g_i()$는 $Z$ 이전의 값들이 주어졌을 때 $Z_i$의 조건부 분포를 임의의 순서로 표현하기위해 파라미터들을 계산합니다.\n",
    "\n",
    "여기서 우리는 문장과 같이 가변적인 길이를 가진 데이터를 다뤄야하기 때문에 위의 접근법이 적용되어야 합니다. 또 다른 중요한 차이점은 모든 $Z_i$(i번째 위치의 단어)는 동일한 타입의 객체(단어)를 참조해야 합니다. 따라서 여기서 제안된 모델은 시간 경과에 따른 매개 변수 공유(같은 시간 경과에 따른 $g_i$), 즉 서로 다른 위치에 있는 단어들에 걸쳐 매개 변수 공유를 도입합니다. \n",
    "\n",
    "우리는 단어간 유사성을 표현하기 위해 각각의 단어에 대한 연속적인 실제 벡터(예를 들어 학습된 분산 피쳐 벡터)를 사용합니다.\n",
    "\n",
    "우리는 자연어 텍스트에서 단어 순서의 확률 분포를 압축적으로 표현하는 데 도움이 되는 단어에 대한 표현을 찾습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Model\n",
    "\n",
    "트레이닝 셋은 매우 크지만 유한한 용어(Vocabulary) 집합 V에 속하는 단어들로 이루어진 $w_1\\cdot \\cdot \\cdot w_T$ 시퀀스입니다. 목적은 표본 외의 데이터에게 높은 확률을 줄 수 있는 좋은 모델 $f(w_t,\\cdot\\cdot\\cdot,w_{t-n+1})=\\hat{P}(w_t|w_1^{t-1})$을 학습하는 것입니다. 아래에서 우리는 perplexity라고 알려진 평균 Negative log-likelihood의 지수, $1/\\hat{P}(w_t|w_1^{t-1})$의 기하학적 평균을 알려드릴것입니다. 이 모델에서 유일한 제약 조건은 $f>0$일 때 어떤 $w_1^{t-1}$에 대해서 $\\Sigma_{i=1}^{|V|}f(i,w_{t-1},\\cdot\\cdot\\cdot,w_{t-n+1})=1$이어야 합니다. 이 조건부 확률들을 곱합으로써 문장의 결합 확률의 모델을 얻을 수 있습니다.\n",
    "1. V의 어떤 요소 i부터 실수 벡터 $C(i)\\in\\mathbb{R}^m$까지 C를 매핑합니다. 용어의 각 단어와 연관된 분산 피쳐 벡터들을 나타냅니다. 실제로 C는 free parameter들의 행렬 $|V|\\times m$ 로 표현됩니다.\n",
    "2. C로 표현되는 단어들에 대한 확률 함수 g는 문맥에서 단어들의 피쳐 벡터의 입력 문장($C(w_{t-n+1}),\\cdot\\cdot\\cdot,C(w_{t-1})$)을 다음 단어 $w_t$에 대한 V의 단어들의 조건부 확률 분산으로 매핑합니다. g의 결과는 Figure 1에서 확률 $\\hat{P}(w_t=i|w_1^{t-1})$을 예측하는 i번째 요소 벡터입니다.\n",
    "\n",
    "<img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/30eb0d43-02f9-4457-aee2-161e462f9e10/_2021-03-04__2.08.26.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20210304%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210304T050920Z&X-Amz-Expires=86400&X-Amz-Signature=80dc002142d2b024cf5facc1b9f6ad1bed84073910fa1b7fe22f16ce87327163&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22_2021-03-04__2.08.26.png%22\" width=400>\n",
    "Figure 1: Neural architecture: $f(i,w_{t−1},···,w_{t−n+1} ) = g(i,C(w_{ t−1 }),··· ,C(w_{t−n+1} ))$ where $g$ is the neural network and $C(i)$ is the i-th word feature vector.\n",
    "\n",
    "함수 $f$는 두 맵핑 (C 와 g)으로 구성되는데, 여기서 C는 문맥에서 모든 단어들 사이에 공유됩니다. 이 둘은 각각 어떤 파라미터들과 연관되어 있습니다. C의 파라미터들은 단순한 피쳐 벡터들로 $|V|\\times m$ 행렬로 표현됩니다. 여기서 $C$의 $i$행은 단어 $i$에 대한 피쳐 벡터 $C(i)$입니다. 함수 $g$는 feed-forward 뉴럴 네트워크, RNN 또는 파라미터 $\\omega$ 파라미터를 가진 함수로 구현됩니다. 전체 파라미터 셋은 $\\theta = (C,\\omega)$입니다.\n",
    "\n",
    "훈련은 penalized log-likelihood를 최대화하는 $\\theta$를 찾도록 수행됩니다.\n",
    "\n",
    "$$L={1\\over{T}}\\Sigma_t{\\log f(w_t,w_{t-1},\\cdot\\cdot\\cdot,w_{t-n+1};\\theta)+R(\\theta)}$$\n",
    "$R(\\theta)$는 Regularization Term입니다. 예를들어, R은 Bias가 아니라, 뉴럴 네트워크의 가중치와 C 행렬에 적용되는 Weight Decay Penalty입니다.\n",
    "\n",
    "위의 모델에서 Free Parameters의 수는 V(사전안에 있는 단어들의 수)에 선형적으로 확장됩니다. 또한 이것은 n에 따라 선형적으로 확장됩니다. 예를들어 time-delay 뉴럴 네트워크나 RNN과 같이 더 많은 공유된 구조가 도입되면 확장 계수가 줄어들 수 있습니다.\n",
    "\n",
    "뉴럴 네트워크는 단어 피쳐 뒤에 하나의 히든 레이어를 가지고 있고, 옵션적으로 단어 피쳐들부터 결과까지의 연결을 잇습니다. 그렇게 실제로는 두개의 히든 레이어가 존재합니다. 비선형성을 가지고 있지 않은 공유되는 단어 피쳐 레이어 C와 일반적인 하이퍼볼릭 탄젠트 히든 레이어입니다. 더 정확하게, 뉴럴 네트워크는 postive 확률들의 합이 1이 되는 것을 보장하는 softmax output 레이어에서 아래 함수를 계산합니다.\n",
    "\n",
    "$$\\hat{P}(w_t|w_{t-1},\\cdot \\cdot \\cdot, w_{t-n+1})={{e^{y_{w_t}}}\\over{\\Sigma_ie^{y_i}}}$$\n",
    "\n",
    "$y_i$는 파라미터 $b, W, U,d,H$들과 함께 아래와 같이 계산되는 각 결과 단어 $i$에 대한 정규화되지 않은 로그 가능성입니다.\n",
    "\n",
    "$y=b+Wx+U\\tanh(d+Hx)$\n",
    "\n",
    "여기서 tanh는 요소별로 적용되며, $W$는 optionally 0이고, $x$는 행렬  C의 입력 단어 피쳐들의 연속인 단어 피쳐 레이어 활성 벡터입니다.\n",
    "\n",
    "$$x=(C(w_{t-1}),C(w_{t-2}),\\cdot\\cdot\\cdot,C(w_{t-n+1}))$$\n",
    "\n",
    "$h$를 히든 유닛의 수라 하고, m을 각 단어와 관련된 피쳐들의 수라고 가정합니다. 단어 피쳐들로부터 결과까지 어떤 연결도 없다면, 행렬 $W$는 0으로 설정됩니다. 모델의 Free Parameter들은 결과 bias $b$, 히든 레이어 bias $d$, 히든부터 결과까지의 가중치 $U$, 결과 가중치에 대한 단어 피쳐 $W$, 히든 레이어 가중치 $H$, 그리고 단어 피쳐 $C$\n",
    "\n",
    "$$\\theta = (b,d,W,U,H,C)$$\n",
    "\n",
    "Free Parameter의 수는 $|V|(1+nm+h)+h(1+(n-1)m)$입니다. 지배인자(Dominating Factor)는 $|V|(nm + h)$입니다. 만약 C를 제외한 가중치 W와 H에 가중치 감소가 있다면, W와 H는 0으로 집중되고, C는 커질 것입니다. 실제로 우리는 Stochastic Gradient Ascent를 사용하여 훈련을 했을 때 이런 현상을 관측하진 않았습니다.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\epsilon {{\\partial \\log \\hat{P}(w_t|w_{t-1},\\cdot\\cdot\\cdot w_{t-n+1}}\\over{\\partial \\theta}}$$\n",
    "\n",
    "여기서 $\\epsilon$은 learning late 라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <a id='ref-1'>Non-Parametric Density Estimation (Non-Parametric 밀도 추정)</a> : 어떠한 사전 정보나 지식 없이 순수하게 관측된 데이터만으로 확률밀도함수를 추정해야 하는 것\n",
    "- <a id='ref-2'>Probability Mass (확률 질량)</a> : 이산확률변수의 확률분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLM\n",
    "\n",
    "\n",
    "[참조 링크](https://wikidocs.net/21687)\n",
    "\n",
    "## 조건부 확률\n",
    "\n",
    "조건부 확률은 두 확률 $P(A)$,$P(B)$에 대해서 아래와 같은 관계를 갖습니다.\n",
    "\n",
    "$$P(B|A)={{P(A,B)}\\over{P(A)}}$$\n",
    "\n",
    "$$P(A,B)=P(A)P(B|A)$$\n",
    "\n",
    "더 많은 확률에 대해서 일반화해봅시다. 4개의 확률이 조건부 확률의 관계를 가질 때, 아래와 같이 표현할 수 있습니다.\n",
    "\n",
    "$$P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$\n",
    "\n",
    "이를 조건부 확률의 연쇄 법칙(chain rule)이라고 합니다. 이제는 4개가 아닌 n개에 대해서 일반화를 해봅시다.\n",
    "\n",
    "$$P(x_1,x_2,x_3...x_n)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1...x_{n−1})$$\n",
    "\n",
    "조건부 확률에 대한 정의를 통해 문장의 확률을 구해보겠습니다.\n",
    "\n",
    "## 문장에 대한 확률\n",
    "\n",
    "문장 'An adorable little boy is spreading smiles'의 확률 $P(\\text{An adorable little boy is spreading smiles})$를 식으로 표현해봅시다.\n",
    "\n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어입니다. 그리고 모든 단어로부터 하나의 문장이 완성됩니다. 그렇기 때문에 문장의 확률을 구하고자 조건부 확률을 사용하겠습니다. 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 문장의 확률은 각 단어들이 이전 단어가 주어졌을 때 다음 단어로 등장할 확률의 곱으로 구성됩니다.\n",
    "\n",
    "$$P(w_1,w_2,w_3,w_4,w_5,...w_n)=\\prod_{n=1}^n{P(w_n|w_1,...,w_{n−1})}$$\n",
    "\n",
    "위의 문장에 해당 식을 적용해보면 다음과 같습니다.\n",
    "\n",
    "$$P(\\text{An adorable little boy is spreading smiles})=\n",
    "P(\\text{An})×P(\\text{adorable|An})×P(\\text{little|An adorable})×P(\\text{boy|An adorable little})×P(\\text{is|An adorable little boy}) ×P(\\text{spreading|An adorable little boy is})×P(\\text{smiles|An adorable little boy is spreading})$$\n",
    "\n",
    "문장의 확률을 구하기 위해서 각 단어에 대한 예측 확률들을 곱합니다.\n",
    "\n",
    "## 카운트 기반의 접근\n",
    "\n",
    "문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률인 $P(\\text{is|An adorable little boy})$를 구해봅시다.\n",
    "\n",
    "$$P(\\text{is|An adorable little boy})={{count(\\text{An adorable little boy is})}\\over{count(\\text{An adorable little boy)}}}$$\n",
    "\n",
    "그 확률은 위와 같습니다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 $P(\\text{is|An adorable little boy})$는 30%입니다.\n",
    "\n",
    "## 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "\n",
    "언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링 합니다. 실제로 정확하게 알아볼 방법은 없겠지만 현실에서도 An adorable little boy가 나왔을 때 is가 나올 확률이라는 것이 존재합니다. 이를 실제 자연어의 확률 분포, 현실에서의 확률 분포라고 명칭합시다. 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면 갖고있는 코퍼스(corpus). 즉, 다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.\n",
    "\n",
    "$$P(\\text{is|An adorable little boy})={{count(\\text{An adorable little boy is})}\\over{count(\\text{An adorable little boy)}}}$$\n",
    "\n",
    "예를 들어 위와 같이 $P(\\text{is|An adorable little boy})$를 구하는 경우에서 기계가 훈련한 코퍼스에 An adorable little boy is라는 단어 시퀀스가 없었다면 이 단어 시퀀스에 대한 확률은 0이 됩니다. 또는 An adorable little boy라는 단어 시퀀스가 없었다면 분모가 0이 되어 확률은 정의되지 않습니다. 그렇다면 코퍼스에 단어 시퀀스가 없다고 해서 이 확률을 0 또는 정의되지 않는 확률이라고 하는 것이 정확한 모델링 방법일까요? 아닙니다. 현실에선 An adorable little boy is 라는 단어 시퀀스가 존재하고 또 문법에도 적합하므로 정답일 가능성 또한 높습니다. 이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 **희소 문제(sparsity problem)**라고 합니다.\n",
    "\n",
    "위 문제를 완화하는 방법으로 n-gram이나 스무딩이나 백오프와 같은 여러가지 일반화(generalization) 기법이 존재합니다. 하지만 희소 문제에 대한 근본적인 해결책은 되지 못하였습니다. 결국 이러한 한계로 인해 언어 모델의 트렌드는 통계적 언어 모델에서 인공 신경망 언어 모델로 넘어가게 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
